<html>
<head>
  <meta name="keywords"
 content="Markov Decision Processes, Toolbox, MATLAB">
  <title>Presentation of MDP toolbox documentation</title>
</head>
<body bgcolor="#ffffff">
<table nosave="" cols="2" width="100%">
  <tbody>
    <tr nosave="">
      <td nosave=""><a href="http://www.inra.fr"><img
 style="width: 209px; height: 104px;" src="INRA.png" border="0"></a></td>
      <td>
      <div align="right"><a href="http://www.inra.fr/mia/"><img
 style="width: 262px; height: 104px;" src="BIA.png" border="0"></a></div>
      </td>
    </tr>
  </tbody>
</table>
<br>
<br>
<br>
<center>
<p><b><font size="+2">Markov Decision Processes (MDP) Toolbox v3.0 for
MATLAB</font></b></p>
</center>
<br>
<br>
<br>
<br>
<p><b>CONTENTS</b>
</p>
<p>The MDP toolbox proposes functions related to the resolution of
discrete-time
Markov Decision Processes: backwards induction, value iteration, policy
iteration,
linear programming algorithms with some variants.
<br>
The functions (m-functions) were developped with <a
 href="http://www.mathworks.com/products/matlab/">MATLAB</a>
(one of the functions requires the <a
 href="http://www.mathworks.com/products/optimization">Mathworks
Optimization Toolbox</a>) by the <a
 href="http://mia.toulouse.inra.fr">Biometry
and Artificial Intelligence Unit</a> of <a
 href="http://www.toulouse.inra.fr">INRA
Toulouse</a> (France).
<br>
The version 2.0 (February 2005) handles sparse matrices and adds an
example.<br>
The version 3.0 (September 2009) adds several functions related to Reinforcement Learning and improve the handling of sparse matrices. For more detail see the README file.<br>
<br></p>
<p><b>FUNCTIONS DESCRIPTION</b>
</p>
<ul>
  <li><font size="+1"> <a href="index_category.html">Functions by
category</a></font></li>
  <p> </p>
  <li><font size="+1"> <a href="index_alphabetic.html">Alphabetical
list of functions</a></font></li>
</ul>
<br>
<p><b>NOTATION</b>
</p>
<ul>
  <li>states: set {1, 2, ..., S}</li>
  <li>actions: set {1, 2, ..., A}</li>
  <li>transitions: P(s,s',a) is the probability to reach state s' when the system is in state s and action a is performed by the decision maker</li>
  <li>rewards: R(s,s',a) is the reward obtained when the system is in state s on decision epoch t and is in state s' at decision epoch t+1, with action a performed<br>
R(s,a): reward when the system is in state s at decision epoch t and
action a is performed by the decision maker</li>
</ul>
<br>
<p><b>USE OF DOCUMENTATION</b>
</p>
<p>The documentation pages can be displayed with the MATLAB navigator (used for
the MATLAB help). In the <i>File</i> menu, choose the <i>Open</i>
item. Open the sub-directory <i>documentation</i>. Select the item
'All Files (*.*)' for the attribut <i>Files of type</i>. Then select
the file <i>DOCUMENTATION.html</i> and open it.
<br>
The sub-directory <i>documentation</i> contains all the pages
describing the m-functions in HTML.
<br>
<br>
<br>
<br>
</p>
<hr width="100%"><font size="-1">mdp_toolbox/documentation/DOCUMENTATION.html
<br>
Page created on July 31, 2009. Last update on August 31, 2009.</font>
</body>
</html>
